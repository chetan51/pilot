# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2013, Numenta, Inc.  Unless you have purchased from
# Numenta, Inc. a separate commercial license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

"""
Template file used by the OPF Experiment Generator to generate the actual
description.py file by replacing $XXXXXXXX tokens with desired values.

This description.py file was generated by:
'~/nta/eng/lib/python2.6/site-packages/nupic/frameworks/opf/expGenerator/ExpGenerator.py'
"""

from nupic.frameworks.opf.expdescriptionapi import ExperimentDescriptionAPI

from nupic.frameworks.opf.expdescriptionhelpers import (
    updateConfigFromSubConfig,
    applyValueGettersToContainer,
    DeferredDictLookup)

from nupic.frameworks.opf.clamodelcallbacks import *
from nupic.frameworks.opf.metrics import MetricSpec
from nupic.frameworks.opf.opfutils import (InferenceType,
                                           InferenceElement)
from nupic.support import aggregationDivide

from nupic.frameworks.opf.opftaskdriver import (
    IterationPhaseSpecLearnOnly,
    IterationPhaseSpecInferOnly,
    IterationPhaseSpecLearnAndInfer)

from pilot.copter import copter_model_params

config = copter_model_params.MODEL_PARAMS

config['numRecords'] = 10000

# Adjust base config dictionary for any modifications if imported from a
# sub-experiment
updateConfigFromSubConfig(config)


# Compute predictionSteps based on the predictAheadTime and the aggregation
# period, which may be permuted over.
if config['predictAheadTime'] is not None:
    predictionSteps = int(round(aggregationDivide(
        config['predictAheadTime'], config['aggregationInfo'])))
    assert (predictionSteps >= 1)
    config['modelParams']['clParams']['steps'] = str(predictionSteps)


# Adjust config by applying ValueGetterBase-derived
# futures. NOTE: this MUST be called after updateConfigFromSubConfig() in order
# to support value-getter-based substitutions from the sub-experiment (if any)
applyValueGettersToContainer(config)
control = {
    # The environment that the current model is being run in
    "environment": 'grok',

    # Input stream specification per py/nupic/cluster/database/StreamDef.json.
    #
    'dataset': {
        u'info': u'test_copter',
        u'streams': [{u'columns': [u'*'],
                      u'info': u'copter.csv',
                      u'last_record': config['numRecords'],
                      u'source': u'file://pilot/copter.csv'}],
        'aggregation': config['aggregationInfo'],
        u'version': 1},

    # Iteration count: maximum number of iterations.  Each iteration corresponds
    # to one record from the (possibly aggregated) dataset.  The task is
    # terminated when either number of iterations reaches iterationCount or
    # all records in the (possibly aggregated) database have been processed,
    # whichever occurs first.
    #
    # iterationCount of -1 = iterate over the entire dataset
    'iterationCount': -1,


    # A dictionary containing all the supplementary parameters for inference
    "inferenceArgs": {'predictedField': config['predictedField'],
                      'predictionSteps': config['predictionSteps']},

    # Metrics: A list of MetricSpecs that instantiate the metrics that are
    # computed for this experiment
    'metrics': [],

    # Logged Metrics: A sequence of regular expressions that specify which of
    # the metrics from the Inference Specifications section MUST be logged for
    # every prediction. The regex's correspond to the automatically generated
    # metric labels. This is similar to the way the optimization metric is
    # specified in permutations.py.
    'loggedMetrics': ['.*aae.*'],
}

# Add multi-step prediction metrics
for steps in config['predictionSteps']:
    control['metrics'].append(
        MetricSpec(field=config['predictedField'], metric='multiStep',
                   inferenceElement='multiStepBestPredictions',
                   params={'errorMetric': 'aae', 'window': 1000, 'steps': steps}))
    control['metrics'].append(
        MetricSpec(field=config['predictedField'], metric='trivial',
                   inferenceElement='prediction',
                   params={'errorMetric': 'aae', 'window': 1000, 'steps': steps}))
    control['metrics'].append(
        MetricSpec(field=config['predictedField'], metric='multiStep',
                   inferenceElement='multiStepBestPredictions',
                   params={'errorMetric': 'altMAPE', 'window': 1000, 'steps': steps}))
    control['metrics'].append(
        MetricSpec(field=config['predictedField'], metric='trivial',
                   inferenceElement='prediction',
                   params={'errorMetric': 'altMAPE', 'window': 1000, 'steps': steps}))


#
#
descriptionInterface = ExperimentDescriptionAPI(modelConfig=config,
                                                control=control)
